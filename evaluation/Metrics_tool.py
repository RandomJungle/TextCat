from sklearn import metrics
import os
import csv

class Metrics_tool:

    def __init__(self, filename):
    	
    	# set the filelist to work from
    	self.directory = os.path.dirname(os.path.realpath(__file__))
    	self.filelist = self.extractfilelist()

    	# a numpy array of class labels generated by human
    	self.y_true = self.extract_array_from_files(self.filelist, index=1)
    	# a numpy array of class labels generated by machine
    	self.y_pred = self.extract_array_from_files(self.filelist, index=2)
    	
    	self.accuracy = metrics.accuracy_score(self.y_true, self.y_pred)
    	self.confusion_matrix = metrics.confusion_matrix(self.y_true, self.y_pred)
    	self.fmeasure = metrics.f1_score(self.y_true, self.y_pred)
        self.precision = metrics.precision_score(self.y_true, self.y_pred)
        self.recall = metrics.recall_score(self.y_true, self.y_pred)
        
    def extractfilelist(self):
    	
    	file_list = []
    	for filename in os.listdir(self.directory):
    		if filename.endswith(".csv") and 'result' in filename: 
    			file_list.append(filename)
    	return file_list

    def extract_array_from_files(self, file_list, index):
    	
    	result = np.array()
    	for file in file_list :
    		with open(self.directory + '\\' + file, 'r', encoding='utf-8') as f:
    			reader = csv.reader(f, delimiter='\t')
    			for row in reader :
    				# eventuellement transformer la classe en valeur num√©raire d'encodage
    				result.append(row[index])
    	return result
    
    def output_graphic():
    	
    	
    
    def save_results():
    	
		with open (self.directory+'\\metrics.txt') as mf :
			mf.write("Results for the pass : " + self.ID_ + '\n\n\n')
			mf.write("Accuracy : " + str(self.accuracy))
			mf.write("F-Mesure : " + str(self.fmeasure))
			mf.write("Precision : " + str(self.precision))
			mf.write("Recall : " + str(self.recall))
			mf.write.(self.confusion_matrix)
    	# from all self data metrics, construct little note on the performance
    	# with or without graphics
    	# save it to file or display it
    	
if __name__ == '__main__':

    tool = Metrics_tool()
    tool.save_results()